{
  "title": "Twitter scraper",
  "type": "object",
  "schemaVersion": 1,
  "properties": {
  "searchTerms": {
  "title": "Do you want to add some search terms?",
  "type": "array",
  "prefill": [
  "Apify"
  ],
  "editor": "stringList",
  "description": "If you add search terms, the scraper will find and extract tweets that mention those terms."
  },
  "handle": {
  "title": "Do you want to add some Twitter handles?",
  "type": "array",
  "description": "You can add the twitter handles of specific profiles you want to scrape. This is a shortcut so that you don't have to add full username URLs like https://twitter.com/username.",
  "editor": "stringList"
  },
  "startUrls": {
  "title": "Do you want to add Twitter URLs to tell the scraper where to start?",
  "description": "You can enter URLs one by one. You can also link to or upload a text file with a list of URLs.",
  "default": [],
  "type": "array",
  "editor": "requestListSources"
  },
  "searchMode": {
  "title": "Do you want to choose the kind of content to scrape?",
  "description": "This setting will change how the data is received by the scraper.",
  "editor": "select",
  "type": "string",
  "prefill": "",
  "enum": [
  "",
  "live",
  "user",
  "image",
  "video"
  ],
  "enumTitles": [
  "Top",
  "Latest",
  "People",
  "Photos",
  "Videos"
  ]
  },
  "mode": {
  "title": "What kinds of tweets do you want to scrape?",
  "description": "You can choose to only scrape a user's own tweets or you can choose to also scrape the user's own tweets and the user's replies to other users.",
  "type": "string",
  "editor": "select",
  "prefill": "replies",
  "default": "replies",
  "enumTitles": [
  "Only own tweets",
  "Own tweets and replies to other users"
  ],
  "enum": [
  "own",
  "replies"
  ]
  },
  "tweetsDesired": {
  "title": "Set the maximum number of tweets",
  "type": "integer",
  "description": "This value lets you set the maximum number of tweets to retrieve. Twitter has a default limit of around 3,200 tweets. Check the README for workarounds.",
  "maximum": 3300,
  "prefill": 100,
  "default": 100
  },
  "addUserInfo": {
  "title": "Add user information",
  "description": "Appends an object to each tweet containing the user information. You can decrease the size of your dataset by turning this off.",
  "default": true,
  "type": "boolean",
  "editor": "checkbox"
  },
  "toDate": {
  "title": "Tweets newer than",
  "description": "Scrape tweets newer than this date. You can use this in conjunction with 'Tweets older than' to create a limited time slice. ",
  "pattern": "(\\d{4}-\\d{2}-\\d{2}|(\\d+)\\s?\\S+)",
  "type": "string",
  "editor": "textfield",
  "sectionCaption": "Do you want to set specific dates?",
  "sectionDescription": "You can choose to scrape only tweets older or newer than a specific date. You can use YYYY-MM-DD format or just relative dates like '1 month' or '2 days'."
  },
  "fromDate": {
  "title": "Tweets older than",
  "description": "Scrape tweets from this date and before. You can use this in conjunction with 'Tweets newer than'to create a limited time slice.",
  "type": "string",
  "pattern": "(\\d{4}-\\d{2}-\\d{2}|(\\d+)\\s?\\S+)",
  "editor": "textfield"
  },
  "proxyConfig": {
  "title": "Proxy configuration",
  "type": "object",
  "description": "This is required if you want to use Apify Proxy.",
  "prefill": {
  "useApifyProxy": true
  },
  "default": {
  "useApifyProxy": true
  },
  "editor": "proxy",
  "sectionCaption": "Proxy configuration",
  "sectionDescription": "Choose which proxies to use."
  },
  "extendOutputFunction": {
  "title": "Extend Output Function",
  "description": "Add or remove properties on the output object or omit the output returning null",
  "type": "string",
  "default": "async ({ data, item, page, request, customData, Apify }) => {\n  return item;\n}",
  "prefill": "async ({ data, item, page, request, customData, Apify }) => {\n  return item;\n}",
  "editor": "javascript",
  "sectionCaption": "Extend scraper functionality",
  "sectionDescription": "You can change the output of the items for your dataset here, or add additional behavior to the scraper."
  },
  "extendScraperFunction": {
  "title": "Extend Scraper Function",
  "description": "Advanced function that allows you to extend the default scraper functionality, allowing you to manually perform actions on the page",
  "type": "string",
  "default": "async ({ page, request, addSearch, addProfile, _, addThread, addEvent, customData, Apify, signal, label }) => {\n \n}",
  "prefill": "async ({ page, request, addSearch, addProfile, _, addThread, addEvent, customData, Apify, signal, label }) => {\n \n}",
  "editor": "javascript"
  },
  "customData": {
  "title": "Custom data",
  "description": "Any data that you want to have available inside the Extend Output/Scraper Function",
  "default": {},
  "prefill": {},
  "type": "object",
  "editor": "json"
  },
  "handlePageTimeoutSecs": {
  "title": "Max timeout seconds",
  "description": "Max timeout for the handlePageFunction. Can be increased for long running processes",
  "default": 5000,
  "prefill": 5000,
  "editor": "number",
  "type": "integer"
  },
  "stealth": {
  "title": "Stealth",
  "description": "Enabling stealth allows to decrease the chance of your scrape being detected as an automated process. We recommend enabling this if you provide your login cookies.",
  "default": false,
  "type": "boolean",
  "editor": "checkbox"
  },
  "maxRequestRetries": {
  "title": "Max request retries",
  "description": "Set the max request retries",
  "default": 3,
  "prefill": 3,
  "type": "integer",
  "editor": "number"
  },
  "maxIdleTimeoutSecs": {
  "title": "Scrolling idle seconds",
  "description": "Configures how many seconds of no data received will be considered done",
  "default": 30,
  "prefill": 30,
  "type": "integer",
  "editor": "number"
  },
  "debugLog": {
  "title": "Debug log",
  "description": "Enable debug log",
  "default": false,
  "type": "boolean",
  "editor": "checkbox"
  },
  "initialCookies": {
  "title": "Login cookies",
  "type": "array",
  "default": [],
  "prefill": [],
  "description": "Your login cookies will be used to bypass the login wall. Check the README for detailed instructions.",
  "editor": "json",
  "sectionCaption": "Login (optional)",
  "sectionDescription": "You will have access to more data if you provide Twitter login information."
  }
  },
  "required": [
  "proxyConfig"
  ]
  }